# üö® CRITICAL ERROR FOUND IN VERCEL LOGS

**Date:** Dec 26, 2025 00:04:17
**Endpoint:** POST /api/chat
**Status:** 200 (but stream error)

## Error Message:

```
[Chat API] ‚ùå Stream error: [Error]: 400 
{
  "error": {
    "message": "The model `llama-3.2-90b-text-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.",
    "type": "invalid_request_error",
    "code": "model_decommissioned"
  }
}
```

## Root Cause:

**The Groq model `llama-3.2-90b-text-preview` has been DECOMMISSIONED by Groq!**

This is why HOLLY isn't responding - the model we're trying to use no longer exists.

## Solution:

We need to update the model name in the chat API route to use a currently supported Groq model.

**Recommended models:**
- `llama-3.3-70b-versatile` (best for general use)
- `llama-3.1-70b-versatile` (alternative)
- `mixtral-8x7b-32768` (faster, good for chat)

## Files to Update:

1. `/app/api/chat/route.ts` - Change model name from `llama-3.2-90b-text-preview` to `llama-3.3-70b-versatile`
2. Any other files that reference the old model name

## Next Steps:

1. Search codebase for `llama-3.2-90b-text-preview`
2. Replace with `llama-3.3-70b-versatile`
3. Commit and deploy
4. Test HOLLY's responses
